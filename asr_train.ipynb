{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ambient-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from common import helpers\n",
    "\n",
    "from common.dataset import AudioDataset, get_data_loader\n",
    "from common.features import BaseFeatures, FilterbankFeatures\n",
    "from common.helpers import (Checkpointer, greedy_wer, num_weights, print_once,\n",
    "                            process_evaluation_epoch)\n",
    "from common.tb_dllogger import flush_log, init_log, log\n",
    "from jasper import config\n",
    "from transducer.model import Transducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boolean-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \n",
    "    model_str = \"./configs/transducer_asr.yaml\"\n",
    "    dataset_str = \"/Users/madhuhegde/work/berkeley/ASR/SpeechRecognition/datasets/LibriSpeech/\"\n",
    "    traindata_str = [dataset_str+\"librispeech-train-clean-100-wav.json\"]\n",
    "    valdata_str = [dataset_str+\"librispeech-dev-clean-wav.json\"]\n",
    "    out_str = \"./results/\"\n",
    "   \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Jasper')\n",
    "\n",
    "    training = parser.add_argument_group('training setup')\n",
    "    training.add_argument('--epochs', default=10, type=int,\n",
    "                          help='Number of epochs for the entire training; influences the lr schedule')\n",
    "   \n",
    "    training.add_argument('--seed', default=42, type=int, help='Random seed')\n",
    "   \n",
    "    optim = parser.add_argument_group('optimization setup')\n",
    "    optim.add_argument('--batch_size', default=1, type=int,\n",
    "                       help='Global batch size')\n",
    "    optim.add_argument('--lr', default=1e-4, type=float,\n",
    "                       help='Peak learning rate')\n",
    "    optim.add_argument(\"--lr_exp_gamma\", default=0.99, type=float,\n",
    "                       help='gamma factor for exponential lr scheduler')\n",
    "   \n",
    "    io = parser.add_argument_group('feature and checkpointing setup')\n",
    "    #io.add_argument('--dali_device', type=str, choices=['none', 'cpu', 'gpu'],\n",
    "    #                default='gpu', help='Use DALI pipeline for fast data processing')\n",
    "    \n",
    "    io.add_argument('--model_config', type=str, default = model_str,\n",
    "                    help='Path of the model configuration file')\n",
    "    io.add_argument('--train_manifests', type=str, default=traindata_str, nargs='+',\n",
    "                    help='Paths of the training dataset manifest file')\n",
    "    io.add_argument('--val_manifests', type=str, default=valdata_str, nargs='+',\n",
    "                    help='Paths of the evaluation datasets manifest files')\n",
    "    io.add_argument('--max_duration', type=float,\n",
    "                    help='Discard samples longer than max_duration')\n",
    "    io.add_argument('--pad_to_max_duration', action='store_true', default=False,\n",
    "                    help='Pad training sequences to max_duration')\n",
    "    io.add_argument('--dataset_dir', default=dataset_str, type=str,\n",
    "                    help='Root dir of dataset')\n",
    "    io.add_argument('--output_dir', type=str, default=out_str,\n",
    "                    help='Directory for logs and checkpoints')\n",
    "    io.add_argument('--log_file', type=str, default=None,\n",
    "                    help='Path to save the training logfile.')\n",
    "    return parser.parse_args(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 0.99\n"
     ]
    }
   ],
   "source": [
    "print(args.lr, args.lr_exp_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instructional-airfare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up datasets...\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    args = parse_args()\n",
    "    \n",
    "  \n",
    "    multi_gpu = 0\n",
    "    args.amp = False\n",
    "    torch.manual_seed(args.seed + 1)\n",
    "    np.random.seed(args.seed + 2)\n",
    "    random.seed(args.seed + 3)\n",
    "\n",
    "    #init_log(args)\n",
    "    #print(args.model_config)\n",
    "   \n",
    "    cfg = config.load(args.model_config)\n",
    "    config.apply_duration_flags(cfg, args.max_duration, args.pad_to_max_duration)\n",
    "\n",
    "    symbols = cfg['labels'] + ['<BLANK>']\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    print_once('Setting up datasets...')\n",
    "    train_dataset_kw, train_features_kw = config.input(cfg, 'train')\n",
    "    train_dataset = AudioDataset(args.dataset_dir,\n",
    "                                 args.train_manifests,\n",
    "                                 symbols,\n",
    "                                 **train_dataset_kw)\n",
    "               \n",
    "    train_loader = get_data_loader(train_dataset,\n",
    "                                       batch_size,\n",
    "                                       multi_gpu=multi_gpu,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=4)\n",
    "    train_feat_proc = FilterbankFeatures(**train_features_kw)\n",
    "\n",
    "    val_dataset_kw, val_features_kw = config.input(cfg, 'val')\n",
    "    val_dataset = AudioDataset(args.dataset_dir,\n",
    "                                   args.val_manifests,\n",
    "                                   symbols,\n",
    "                                   **val_dataset_kw)\n",
    "    val_loader = get_data_loader(val_dataset,\n",
    "                                     batch_size,\n",
    "                                     multi_gpu=multi_gpu,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=4,\n",
    "                                     drop_last=False)\n",
    "    \n",
    "    val_feat_proc = FilterbankFeatures(**val_features_kw)\n",
    "    dur = train_dataset.duration / 3600\n",
    "    dur_f = train_dataset.duration_filtered / 3600\n",
    "    nsampl = len(train_dataset)\n",
    "\n",
    "    num_inputs = train_features_kw['n_filt']\n",
    "    model = Transducer(num_inputs, 32)\n",
    "    lr = args.lr\n",
    "    lr_gamma = args.lr_exp_gamma\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_gamma)\n",
    "        \n",
    "        \n",
    "    num_epochs = 0\n",
    "    while(num_epochs>0):\n",
    "        print(num_epochs)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_samples = 0\n",
    "        test_loss = 0\n",
    "        for batch in train_loader:\n",
    "            audio, audio_lens, txt, txt_lens = batch\n",
    "            feat, feat_lens = train_feat_proc(audio, audio_lens, args.amp)\n",
    "            print(feat.shape[0])\n",
    "            feat = feat.transpose(1, 2)\n",
    "            feat = feat.to(model.device)\n",
    "            txt = txt.to(model.device)\n",
    "            batch_size = feat.shape[0]\n",
    "            loss = model.compute_loss(feat,txt,feat_lens,txt_lens)\n",
    "            \n",
    "            num_samples += batch_size\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_loss /= num_samples\n",
    "            break\n",
    "        num_epochs = num_epochs - 1    \n",
    "          \n",
    "        lr_scheduler.step()  \n",
    "        num_samples = 0\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            audio, audio_lens, txt, txt_lens = batch\n",
    "            feat, feat_lens = val_feat_proc(audio, audio_lens, args.amp)\n",
    "            feat = feat.transpose(1, 2)\n",
    "            \n",
    "            feat = feat.to(model.device)\n",
    "            txt = txt.to(model.device)\n",
    "            batch_size = feat.shape[0]\n",
    "            loss = model.compute_loss(feat,txt,feat_lens,txt_lens)\n",
    "            \n",
    "            num_samples += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_loss /= num_samples\n",
    "            break\n",
    "            \n",
    "        num_epochs -= 1    \n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    " symbols = cfg['labels'] + ['<BLANK>']\n",
    " print(symbols[len(symbols)-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(a[31][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.encoder(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1)\n",
    "y = x.unsqueeze(-1).pow(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xx, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,2)\n",
    "x = torch.ones(shape)\n",
    "xx =  x.unsqueeze(0)\n",
    "print(x)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
